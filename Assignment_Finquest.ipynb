{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1HCjR0vWowc"
      },
      "source": [
        "# Profile matching across two sources\n",
        "\n",
        "## Objective\n",
        "\n",
        "Please find attached two files (`source_f.csv` and `source_l.json`), your objective is to:\n",
        "\n",
        "1.\tanalyze these two datasets, both of which are company information from two different real-world sources\n",
        "2.\tpropose as many solutions/steps as possible that: for each row in `source_f.csv`, find a matching row from `source_l.json`\n",
        "3.\toutput a final csv file with the following two columns: `_id` and `company_id` (`_id` is from `source_f.csv` and `company_id` from `source_l.json`)\n",
        "\n",
        "## Sample output\n",
        "\n",
        "```\n",
        "_id,company_id\n",
        "9d3ae4d9-396f-44b6-8d30-0007ab49f838,24748\n",
        "9aa00d4d-8ac1-4eef-827a-0c9b4ff48108,15294012\n",
        "5e02ae26-d3d5-46d2-9359-65a186a4f511,15315906\n",
        "...\n",
        "```\n",
        "\n",
        "## Response\n",
        "\n",
        "Please send **only your Jupyter Notebook** with code and documentation for this task **by email** to daoyuan.li@finquest.com, with the subject line **\"Data Engineer Test: CANDIDATE-NAME\"**.\n",
        "\n",
        "Please also specify how many hours it took you to finish this task in your email.\n",
        "\n",
        "**Please note that emails not confirming to the above format will be ignored.**\n",
        "\n",
        "## Notes\n",
        "\n",
        "* Work on this task independently\n",
        "* Never share this notebook nor the datasets with anyone else, don't post this task on the Internet\n",
        "* Be structured on the steps you take to solve this task\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-14T09:32:21.699604Z",
          "start_time": "2022-03-14T09:32:21.378362Z"
        },
        "id": "3aoJNuBXWowj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_f = pd.read_csv('/content/source_f.csv')"
      ],
      "metadata": {
        "id": "teYKwnsaeYaE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f.head()"
      ],
      "metadata": {
        "id": "T4EHK3C8eElW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f.info()"
      ],
      "metadata": {
        "id": "4TiENl5-dC87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f.describe()"
      ],
      "metadata": {
        "id": "8c1pOgkldcGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l = pd.read_json('/content/source_l.json')"
      ],
      "metadata": {
        "id": "MO-FG4xRdm-K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l.head(5)"
      ],
      "metadata": {
        "id": "vHcPmLO7fVlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l.info() , df_f.info()"
      ],
      "metadata": {
        "id": "IAsaecwBe75-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "total no of records in file  - 8778\n",
        "\n",
        "total no of records in json - 5000\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JNCIt6pQhgMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" trying to find out unique values in each columns and compare the two datasets based on columns \"\"\"\n",
        "print(\"count of unique ids present in csv and json \\ncsv file -  {} \\njson file - {} \".format(len(df_f['_id'].unique()) , len(df_l['company_id'].unique())))"
      ],
      "metadata": {
        "id": "bwzgT86Ngt6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_f['Name'].unique() ,df_l['name'].unique())"
      ],
      "metadata": {
        "id": "9sqqWt-zguC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_f['Name'].unique()) ,len(df_l['name'].unique()))"
      ],
      "metadata": {
        "id": "JmtqGT3zsQGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_f['Name'].unique()) ,len(df_l['name'].unique()))"
      ],
      "metadata": {
        "id": "swaDhtEWvXua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" checking  name match between two dataframe (by converting every name to lower case )  \"\"\"\n",
        "list_file=df_f['Name'].unique()\n",
        "list_json = df_l['name'].str.lower().unique()\n",
        "count=0\n",
        "for i in range(len(df_f['Name'].unique())):\n",
        "  if list_file[i].lower() in list_json:\n",
        "    count+=1\n",
        "    print(list_file[i])\n",
        "print(count)"
      ],
      "metadata": {
        "id": "HLSX04H7sb8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" checking  name match between two dataframe (without converting  name to any case )  \"\"\"\n",
        "list_file=df_f['Name'].unique()\n",
        "list_json = df_l['name'].unique()\n",
        "count=0\n",
        "for i in range(len(df_f['Name'].unique())):\n",
        "  if list_file[i] in list_json:\n",
        "    count+=1\n",
        "    print(list_file[i])\n",
        "print(count)"
      ],
      "metadata": {
        "id": "lh7ppRVBuXNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"creating key columns in both the data frames for join operation , \"\"\"\n",
        "## the reason behind pulling  name and city from both the data frame is becoz these are not null columns\n",
        "df_f['Name_key'] = df_f['Name'].str.lower()\n",
        "df_l['Name_key'] = df_l['name'].str.lower()\n",
        "df_f['City_key'] = df_f['City'].str.lower()\n",
        "df_f['Country_key'] = df_f['Country'].str.lower()\n",
        "df_l['City_key'] = df_l['headquarters'].str.lower().str.split(',').str[0].str.strip()"
      ],
      "metadata": {
        "id": "-0zqPAgwuXQS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l"
      ],
      "metadata": {
        "id": "djxoVgt76Oy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_f['Name'].unique()) ,len(df_l['name'].unique()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCQg8bHFvAgA",
        "outputId": "a3176f7b-5abe-4fd0-c49f-cd539e8cb613"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6984 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# merginng the data frames\n",
        "\n",
        "s1 = pd.merge(df_f, df_l, how='inner', on=['Name_key','City_key'])\n",
        "s1"
      ],
      "metadata": {
        "id": "2GkB25CtguFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(s1['_id'].unique()) ,len(s1['company_id'].unique())\n",
        "## as per my understanding the key relationship was one to many where one company_id can be mappped to many _id\n"
      ],
      "metadata": {
        "id": "eQBUE01H_7jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1.to_csv('output.csv',header=True,index=False,columns=['_id','company_id'])"
      ],
      "metadata": {
        "id": "qtU6SZYk95zJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-14T09:32:21.747163Z",
          "start_time": "2022-03-14T09:32:21.700630Z"
        },
        "id": "jbVh8h_7Wowl"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "matched = {\n",
        "\n",
        "    \"9d3ae4d9-396f-44b6-8d30-0007ab49f838\": 24748,\n",
        "    \"9aa00d4d-8ac1-4eef-827a-0c9b4ff48108\": 15294012,\n",
        "    \"5e02ae26-d3d5-46d2-9359-65a186a4f511\": 15315906\n",
        "}\n",
        "\n",
        "for _id, company_id in matched.items():\n",
        "    display(df_f[df_f['_id'] == _id])\n",
        "    display(df_l[df_l['company_id'] == company_id])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KwUE6xTk_6Pq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now trying to perform same join operation using pyspark"
      ],
      "metadata": {
        "id": "7XO-kFivE8Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###setting up spark in colab environment\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://mirrors.estointernet.in/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "Axub2ToSc8PM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "Meo1zt67FBf_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "1zIOZHvhFKjq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QXV_JFX-FOyN",
        "outputId": "232d1fc1-785d-4345-ed3a-e919198a5372"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.1.2-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Finquest_assignment\").config('spark.ui.port', '4050').getOrCreate()"
      ],
      "metadata": {
        "id": "rIV6RvLDFUfe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "DJOOsHdmFXm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f1 = spark.read.csv(\"source_f.csv\",header=True, mode=\"DROPMALFORMED\")"
      ],
      "metadata": {
        "id": "qBP0b0qHFiTC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f1.show()\n",
        "df_f1.count()"
      ],
      "metadata": {
        "id": "nviogvu0GEmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l1 = spark.read.option(\"multiline\",\"true\").json(\"source_l.json\")"
      ],
      "metadata": {
        "id": "ksuxkhx0GPRG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l1.show()\n",
        "df_l1.count()"
      ],
      "metadata": {
        "id": "_6m7S239GhIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lower, col,split\n"
      ],
      "metadata": {
        "id": "kP6UPqOtRuCJ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f2=df_f1.withColumn(\"Name\", lower(df_f1.Name))\n",
        "df_l2 = df_l1.withColumn(\"name\",lower(df_l1.name))\n",
        "df_f2 = df_f2.withColumn(\"City\",lower(df_f2.City))\n",
        "\n",
        "df_l2  =df_l2.withColumn(\"headquarters\",lower(split(df_l2.headquarters,',')[0]))"
      ],
      "metadata": {
        "id": "ntFcMbQsOqqe"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f2.show()"
      ],
      "metadata": {
        "id": "NLxsCPc6V45s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_l2.show()"
      ],
      "metadata": {
        "id": "GZ6EEJzwVQZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s2= df_f2.join(df_l2, [df_f2.Name == df_l2.name,df_f2.City == df_l2.headquarters], 'inner').select(df_f2._id, df_l2.company_id)"
      ],
      "metadata": {
        "id": "afXjB8DAMttO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s2.show()"
      ],
      "metadata": {
        "id": "AT532x8KGiYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s2.count()"
      ],
      "metadata": {
        "id": "ZUxf4J5eYd5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s2.write.options(header='True',delimiter =',').csv(\"/content/output_spark/\")"
      ],
      "metadata": {
        "id": "2cO-fWZFYs7Y"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azn4ZsJfZ7ha"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}